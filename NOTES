### Setup: 
- https://gist.github.com/actsasgeek/19952660399be0c6dbb0407fe8c56ec4
- python -m ipykernel install --user --name en605645 --display-name "Python (en605645)"
- conda activate en605645

### Oreintation:
- Modules start monday morning
- Self Check and Programing Assignment each week
- Lectures followed by quiz, take notes
- Do readings (do not need to read as if being quized on it), look at book examples. 
- Thursday night self-check due - goal is to prime for programing assignment
- Thursday night office hours (8:00) 
- Friday/Saturday: 2 Peer reviews of other student self-checks
- Programing assignment and quiz are do Sunday night 
- Do quiz earlier (wed/thur)
- Quizes include material from ealrier lectures
- Functions should not be more than 20 executable lines 
- Dont need unit tests for A* (but write why? - since its being tested below)
- make sure it runs in enviroment
- extended peer review until noon sunday


Module 1: Background history of AI
- Types of AI:
	- System AI: Focus on agents that perceive and act in an enviroment. Reasoning, Decision Making
	- Internet AI: ML model embedded in a larger system to do a specific task on structered input, produce structured output (spam filter, recommdation engine, spam filter)

- 2x2 Matrix of What is Systems AI? (4 views)
	- 1) thinks like humans | 2) act like humans: Imitate human behavior
	- 3) think rationally | 4) act rationally: Maxamize performance or metric
	- Tradeoffs!
	- R&N use (4) as "standard model of ai"
- Agents: 
	- perception -> action 

- Standard Model of AI: Rational agents chooses the action that maximizes expected performance. Uses infomation (beliefs), objectives (utility/metric) and actions (policies). Handle uncertainty and trade-offs

- Refinements to Standard Model: 
	- Bounded Rationality: Approxamation of rationality 
	- Uncertain human objectives: fully known objectives are unrealistic so systems should benifit humans while being uncertain about true objectives.

- Philisophical roots of AI: 
	- reasoning as symbol manipulation, symbols = internal represnetions of things. 

- Mathmatical Foundation: 
	- logic, probability and compution
	- Bridges logic and compution

- Inspiration from biology/neuroscience and psychology: 
	- more of a metaphor, not actually exact.

- Linguistics: formal structure, NLP, language as infomation proccessing. (LLMs)

- Control Theory: 
	- predates AI, study of optimal actions


- History of AI: 
	- Cycle of hype, disapoitnment, renewed advances. 
	- Symbolic -> knowledge-based  | Probabalistic -> ML and data | Deep learning resurgance (LLMs)

- Ethics: 
	- No doctrine or guardrails for protecting ethics of developer (unlike doctor) 
	- privacy, misuse, control.
	- ethics and control should be designed upfront
	- corrigibility: the quality of being capable or willing to be corrected
- Disco Prompts: 
	- Where would LLMs be in the 2x2 matrix of system type: think reationliy but act as human
	- "Fun", like in a video game. 
	- LLM cycle, part of "Big Data" cycle - maybe replaced by contintues data model (bigger data) or smaller data model (small, specific training) or non-data based model altogether


Module 1: Agents: 
- An agent perceives (percepts) and act in an an enviroment via actuators. 
- think of a robot 
- Agents Functions map percept to actions, can be determintic or stochastic (same or diff response to same percept). Abstract definitions diff from impl
- Permfomance Measure: qiuantify successs (what is rational action in env)
- Agent: chooses function to get maximum performance. 
- PEAS - Task Enviroment: 
	- Permfomance Measure: Metric
	- Enviroment: What/Where
	- Actuators: Output
	- Sensors: Input

- Env vary in multple ways: 
	- Observable: can see full system state
	- Episodic vs sequential: do percetps and actions feedback into eachother (sequential) or is a the agent used for a single episode of percept->action
- Types of Agents: 
	- Simple reflex agent: responds directly to percept, no internal state, works in fully observable and simple env. (also episodic). Based on condiont action rules
	- Model Based Agent: has state to track unobserved aspects. Uses model of env to inform decisions.
	- Goal Based Agents: acts to achieve explicitly defined goal. Sequential. MOre flexible. 
	- Utility Based Agents: maxamizes expected utility. Handles trade-offs
	- Learning Agents: improve performance over time. Many components. Usefull in changing or unknown env. 
	- ML is not learning agents since ML is trained before hand not "on the job"
	- Generative AI expands traidtional concepts (not in these types)

Module 1: Ethics
- Weak vs Strong AI: 
	- Weak: machines can only ever simulate intelligent behavior, functionally useful 
	- Strong AI: AI can have conciousness, understanding and intentionality

- Turing reframed this question in terms of behavior: can someone distinguesh btw human and machine
- LLMs (informally) passed turing test. 
- Consciousness is still a mystery in people (therefore also ai)
- Ethics: resposibilty to reduce harm. Profesional codes, norms. Need proactive saftey.
- Fairness: does everyone equal access to AI tools. Does AI fairly represent everybody.

Module 1: Algorithms
- When to use AI?
	- When a graph algorthim wont work 
	- when patterns must be learnt or infered
	- clarify missing relationships
	- NLP 
	- Manage preferences
	- Learning 
	- adapt to dynmaic constraints
	- "Fuzzy"
	- System!

- Common classic algorthims w AI flavor (gray area): Union-find, DFS, binary Search, Tarjas algorithim, Longest common subseq, Dijkstras, Bellman-Ford, Suffix-trees, LP
- Dynamic Programming is AI


- Which AI to use?
	- Symbolic AI: reason through rules, constraintd or search in a structered env
	- Discriminative ML: Predict outcome
	- Generative ML: transform data from type A to type B



Module 2: State Space Search

- general approuch to problem solving: turn problem into graph and then form the solution as a path through the graph

- 4 key elements: States, Actions, Transitions, and cost.
	- States are the way of representing the problem space (set of values)
		- Initial state S~0~ -> starting configuration of problem 
		- Goal state S~g~ -> succesful termination state
		- Failure state S~f~ -> non succesful termination state
	- Actions (a): set of things an agent can do to change the state
	- Transitions (T(s,a)): rules for moving between states based on current state and action taken 
	- Cost c(s,a,a`): associated cost of action 

- Succesor function: given a state get all transtion states

- Composing problems in terms of the 4 elements provides a framework for solving them using a search algorithm over the state graph of the problem 

- Best to specify problem in terms of simple and basic actions/transtions and to encode states in a machine freindly form (so u can implement it) 

- branching factor (b): average number of actions per state
- depth (d): depth of the smallest solution
- max (m): maximum depth of the search tree 

- Search algs: 
	- DFS: LIFO (stack), incomplete (not all paths searched), non-opitmal (bc not complete), Space: O(bm) | Time O(b^m)
	- BFS: FIFO (queue) , complete (all nodes are reached), is optimal if costs are uniform, Space O(b^d) | Time O(b^m) - more space 
	- Iterative deepening: search every depth of tree one depth at a time using DFS -> complete so optimality is ensured


- Informed Search: use cost to guide search 
	- Uniform Cost Search (UCS):  
		- use a priority queue to track "frontier" (queue in bfs), using "total path cost" as key so we traverse in order of low->high cost
		- Succesor function returns states based on the transition funciton and cost calculation
		- complete (searches all nodes)
		- optimal even in varying action cost problems
		- same space/time requirements as BFS (bc it is bfs) 

- Greedy (best first search):
	- pick cheapest action at each state as hueristic for getting closer to the goal - using only the cost of the action (unlike UCS which uses path cost)
	- use priority queue to hold state costs
	- admissible huerstic funciton: a function where the huerstic is always less than or equal to the true cost
	
- A* search: 
	- use f(n) = g(n) + h(n) as the key in the priority queue
	- g(n) is the path cost to the node (UCS)
	- h(n) is a huerisitc for how much closer to the goal this node is  (greedy)
	- optimal and complete 
	- If theres a path to the goal it will expand as few nodes as possible (or as well as the hueristic is)
	- 


