### Setup: 
- https://gist.github.com/actsasgeek/19952660399be0c6dbb0407fe8c56ec4
- python -m ipykernel install --user --name en605645 --display-name "Python (en605645)"
- conda activate en605645

### Orientation:
- Modules start monday morning
- Self Check and Programing Assignment each week
- Lectures followed by quiz, take notes
- Do readings (do not need to read as if being quized on it), look at book examples. 
- Thursday night self-check due - goal is to prime for programing assignment
- Thursday night office hours (8:00) 
- Friday/Saturday: 2 Peer reviews of other student self-checks
- Programing assignment and quiz are do Sunday night 
- Do quiz earlier (wed/thur)
- Quizes include material from ealrier lectures
- Functions should not be more than 20 executable lines 
- Dont need unit tests for A* (but write why? - since its being tested below)
- make sure it runs in enviroment
- extended peer review until noon sunday


# Module 1: Background history of AI
- Types of AI:
	- System AI: Focus on agents that perceive and act in an enviroment. Reasoning, Decision Making
	- Internet AI: ML model embedded in a larger system to do a specific task on structered input, produce structured output (spam filter, recommdation engine, spam filter)

- 2x2 Matrix of What is Systems AI? (4 views)
	- 1) thinks like humans | 2) act like humans: Imitate human behavior
	- 3) think rationally | 4) act rationally: Maxamize performance or metric
	- Tradeoffs!
	- R&N use (4) as "standard model of ai"
- Agents: 
	- perception -> action 

- Standard Model of AI: Rational agents chooses the action that maximizes expected performance. Uses infomation (beliefs), objectives (utility/metric) and actions (policies). Handle uncertainty and trade-offs

- Refinements to Standard Model: 
	- Bounded Rationality: Approxamation of rationality 
	- Uncertain human objectives: fully known objectives are unrealistic so systems should benifit humans while being uncertain about true objectives.

- Philisophical roots of AI: 
	- reasoning as symbol manipulation, symbols = internal represnetions of things. 

- Mathmatical Foundation: 
	- logic, probability and compution
	- Bridges logic and compution

- Inspiration from biology/neuroscience and psychology: 
	- more of a metaphor, not actually exact.

- Linguistics: formal structure, NLP, language as infomation proccessing. (LLMs)

- Control Theory: 
	- predates AI, study of optimal actions


- History of AI: 
	- Cycle of hype, disapoitnment, renewed advances. 
	- Symbolic -> knowledge-based  | Probabalistic -> ML and data | Deep learning resurgance (LLMs)

- Ethics: 
	- No doctrine or guardrails for protecting ethics of developer (unlike doctor) 
	- privacy, misuse, control.
	- ethics and control should be designed upfront
	- corrigibility: the quality of being capable or willing to be corrected
- Disco Prompts: 
	- Where would LLMs be in the 2x2 matrix of system type: think reationliy but act as human
	- "Fun", like in a video game. 
	- LLM cycle, part of "Big Data" cycle - maybe replaced by contintues data model (bigger data) or smaller data model (small, specific training) or non-data based model altogether


## Agents: 
- An agent perceives (percepts) and act in an an enviroment via actuators. 
- think of a robot 
- Agents Functions map percept to actions, can be determintic or stochastic (same or diff response to same percept). Abstract definitions diff from impl
- Permfomance Measure: qiuantify successs (what is rational action in env)
- Agent: chooses function to get maximum performance. 
- PEAS - Task Enviroment: 
	- Permfomance Measure: Metric
	- Enviroment: What/Where
	- Actuators: Output
	- Sensors: Input

- Env vary in multple ways: 
	- Observable: can see full system state
	- Episodic vs sequential: do percetps and actions feedback into eachother (sequential) or is a the agent used for a single episode of percept->action
- Types of Agents: 
	- Simple reflex agent: responds directly to percept, no internal state, works in fully observable and simple env. (also episodic). Based on condiont action rules
	- Model Based Agent: has state to track unobserved aspects. Uses model of env to inform decisions.
	- Goal Based Agents: acts to achieve explicitly defined goal. Sequential. MOre flexible. 
	- Utility Based Agents: maxamizes expected utility. Handles trade-offs
	- Learning Agents: improve performance over time. Many components. Usefull in changing or unknown env. 
	- ML is not learning agents since ML is trained before hand not "on the job"
	- Generative AI expands traidtional concepts (not in these types)

## Ethics
- Weak vs Strong AI: 
	- Weak: machines can only ever simulate intelligent behavior, functionally useful 
	- Strong AI: AI can have conciousness, understanding and intentionality

- Turing reframed this question in terms of behavior: can someone distinguesh btw human and machine
- LLMs (informally) passed turing test. 
- Consciousness is still a mystery in people (therefore also ai)
- Ethics: resposibilty to reduce harm. Profesional codes, norms. Need proactive saftey.
- Fairness: does everyone equal access to AI tools. Does AI fairly represent everybody.

## Algorithms
- When to use AI?
	- When a graph algorthim wont work 
	- when patterns must be learnt or infered
	- clarify missing relationships
	- NLP 
	- Manage preferences
	- Learning 
	- adapt to dynmaic constraints
	- "Fuzzy"
	- System!

- Common classic algorthims w AI flavor (gray area): Union-find, DFS, binary Search, Tarjas algorithim, Longest common subseq, Dijkstras, Bellman-Ford, Suffix-trees, LP
- Dynamic Programming is AI


- Which AI to use?
	- Symbolic AI: reason through rules, constraintd or search in a structered env
	- Discriminative ML: Predict outcome
	- Generative ML: transform data from type A to type B



# Module 2: State Space Search

- general approuch to problem solving: turn problem into graph and then form the solution as a path through the graph

- 4 key elements: States, Actions, Transitions, and Cost. [SATC]
	- States are the way of representing the problem space (set of values)
		- Initial state S~0~ -> starting configuration of problem 
		- Goal state S~g~ -> succesful termination state
		- Failure state S~f~ -> non succesful termination state
	- Actions (a): set of things an agent can do to change the state
	- Transitions (T(s,a)): rules for moving between states based on current state and action taken 
	- Cost c(s,a,a`): associated cost of action 

- Succesor function: given a state get all transtion states

- Composing problems in terms of the 4 elements provides a framework for solving them using a search algorithm over the state graph of the problem 

- Best to specify problem in terms of simple and basic actions/transtions and to encode states in a machine freindly form (so u can implement it) 

- branching factor (b): average number of actions per state
- depth (d): depth of the smallest solution
- max (m): maximum depth of the search tree 

- Search algs: 
	- DFS: LIFO (stack), incomplete (not all paths searched), non-opitmal (bc not complete), Space: O(bm) | Time O(b^m)
	- BFS: FIFO (queue) , complete (all nodes are reached), is optimal if costs are uniform, Space O(b^d) | Time O(b^m) - more space 
	- Iterative deepening: search every depth of tree one depth at a time using DFS -> complete so optimality is ensured


- Informed Search: use cost to guide search 
	- Uniform Cost Search (UCS):  
		- use a priority queue to track "frontier" (queue in bfs), using "total path cost" as key so we traverse in order of low->high cost
		- Succesor function returns states based on the transition funciton and cost calculation
		- complete (searches all nodes)
		- optimal even in varying action cost problems
		- same space/time requirements as BFS (bc it is bfs) 

- Greedy (best first search):
	- pick cheapest action at each state as hueristic for getting closer to the goal - using only the cost of the action (unlike UCS which uses path cost)
	- use priority queue to hold state costs
	- admissible huerstic funciton: a function where the huerstic is always less than or equal to the true cost
	
- A* search: 
	- use f(n) = g(n) + h(n) as the key in the priority queue
	- g(n) is the path cost to the node (UCS)
	- h(n) is a huerisitc for how much closer to the goal this node is  (greedy)
	- optimal and complete 
	- If theres a path to the goal it will expand as few nodes as possible (or as well as the hueristic is)
	- Point of using combined priority is to use fewest nodes possible -> avoid loops or dead ends and head towards goal 

# Module 3: Logistic Regression

- Good old fashion AI [GOAI]:
	- problem solving, general purpose, operational, engineering 
- Machine Learning [ML]:
	- Pattern discovery 
	- specific / tailored to problem 
	- representational -  represent pattern
	- data driven!!!!!!
- Catagories of ML: 
	- Supervised 
		- known features and known output
	- Unsupervised 
		- known features, dont know what they represent
	- Reinforcement Learning [RL]

- 2 typsed of Supervised Learning: 
	- Regression: numerical output
		- predict/find pattern between some input(s) and some numeric output - so that given a new input we can predict the output 
		- Types of Algorithms:
			- Linear regression
			- Artificial nureal networks
			- KNN
			- fuzzy controls
			- regression trees

	- Classification: label output 
		- predict label (non-numeric) from feature given data set
		- Types of algorithms:
			- Logistic regression
			- SVM
			- decision trees
			- baysian networks 
			- KNN


- [Features]: input to ML:
	- continues: numeric measurements
	- discrete : numerical counts, rankings dates or catagorical data 
- diff algs are better w diff features - but can coerce (encode) data types to fit algs
- each alg has tradeoffs

- Linear Regression: 
	- y = mx + b : m = slope (dy/dx) , b = y-intercept
	- goal is to find a line that fits the points
	- but how to we find that line: 
		- Gradiant Decent: 
			- Least Mean squared error [MSE]: for n observations of (x,y): MSE = J(m,b) = (1/n) Σ (f(x - y)^2 (use f(x) = mx+b for a given m and b and compare that to the observed y)
			- we call f(x) y-hat
			- plotting MSE over different slope values produces a parabula for linear functions
			- in the case of more than one input feature:
				- we refer to the slopes for each input x1, x2, ... as theta1, theta2 ... (linear coefs)
				- assume there is an x0 that is always 1 then we can write: Θx = yhat, where theta and x are vectors of slopes and features and y-hat is a vector of estimates 
				- J(Θ) = (1/2)(1/n) Σ (yhat(i) - y(i))^2 , for i feauters
				- Goal of alg is to slowly update Θj until we converge on a global minimum: 
					- for a learning rate α: if too big alg wont converge, if too small it will take a while
					- Θ(j) = Θ(j) - α (dj/dΘ(j)) : ie update theta until we get a global minimum 
					-  (dj/dΘ(j)) = (1/n) × Σ(ŷᵢ - yᵢ) × xᵢⱼ

				- gradiant decent is better than an exact solution bc its faster for large data 



Tips for Regrssion: 
	- Mean Nomrmilization: normalize values (features) to mean: x = x - mean/ range 
		- need to de-normalize prefictions/output 
	- Binary encoding: make catagory 0 or 1- also called dummy or indicator variables - turns the regression into different effective intercepts (with the same slope)
	- converting catagorical data into numerical isnt good, instead use bitmask to encode all the catagories of an observation - ie 0010 says observation is part of catagory 1 and not any others 


Logistic Regression: classification
	- One possible alg: clamp linear regression to 0 or 1 using some threshold - not good bc a single observation can throw off the threshold
	- instead use sigmoidal function : yhat = 1/1+e^-z = f(x) where z is the regression formula 
	- yhat ranges from 0->1 and is the probability x belongs to the class. then classify by some threshold 
	- use log as error function:
		- error = y * yhat + (1 - y) * log(1-yhat)
		- J(theta) = -1/n  Σ ylog(yhat) + (1-y) log(1 - yhat) for each obs i
	- if threre are 3 or more catagories we train a clasifiers on each catagory and take the largest prob output as the asnwer

Support Vector Machine: 
	- discriminant: a way to draw a distinction btw two things
	- hinge loss fn: J(theta) = C Σ y* cost(yhat) + (1-y) * cost(1 - yhat)
		- C controls how we deal w outliers: Larger -> more penalized 
	 - SVM is a linear discriminant that used the hinge loss fn as its classifier 
	 - Support vectors are the lines that discriminate between two clusters (at least 2) and the margin is the space btw the suppport vectors and the classifying line
	 - SVMS are solved w quadratic programming (optimization) but can use gradiant decent 
	 - Concept == class in a classification problem. A class can be linearly seperable if it can be perfectly described by a hyperplane, else it is non-linearly seperable.
	- Use a kernel function to project data into a higher dimension and then seperate in that dimension space

Model Evaluation: 
	- how well does a model perfrome on unseen instances?
	- is thre a better model?
	- ML assumes that unseen data is like the things seen already - representative feature set
	- Permfomance Metrics: 
		- MSE (mean squared error) - good for regression 
		- Confusion matrix: good for classification
			- TP: predicted 1 and is 1
			- TN: predicted 0 and is 0
			- FP: preficted 1 and is 0
			- FN: predicted 0 but is 1
		- accuracy = TP+TN / n
		- error = FN + FP / n 
		- accuracy and error arent great metrics since:
			- FP and FN can be weighted differently, one can be "worse" than the other (cancer test) overwarn vs underwarn
			- classes may have unequal distribution in the test data (easy to always predict 1 or 0)
		- recall = TP / TP + FN -> out of all the data how many positives did we get 
		- precision = TP / Tp + FP  -> out of all the positives predicted, how many were correct 
		- TP + FN = number of positives in actuality 
		- tradeoff btw precision and recall

Cross Validation: 
	- randomly split data into sets (called folds). Then train on all-but-1 fold and test on the left out fold. 

Fixing Models: 
	- how do we reduce error?
	- getting more data is not always the solution
	- Bias-Variance tradeoff: 
		- how model performs on training set vs test set 
		- high-bias => model under fits the input data 
		- low-bias => model over-fits the input data 
		- low variance => low prediction error
		- high-variance => high error of prediction 
	- bias =>  how much the input data/model influences the output
	- train on n% of the training set and calculate metrics then train on the test set and calc metric, repeat with increasing % of input data. Error should decrease as we include more data in the training set
	- if the error converges then it is high bias 
		- more data wont help
		- add more features
		- add feature transformations
	- if error doesnt converge then it is high variance: 
		- add more data
		- reucde features
		- remove transformations

Review: 
	- n = number of rows ie observations
	- m is number of features in each observations

# Module 4: Local Search: 
- allows us to solve problems even if we dont know how to get to the local, but can verify a goal. 
- instead of state space search (transitions) we use an evaluation function to drive the search towards the solution 
- can be used when evaluation fn doesnt have a well-defined derivitave (requireed for gradiant decent)
- Hill Climbing: 
	- pick a change that makes the evaluation search better
	- can be min or max problem 
	- can end up in local optimum - (local max/min) 
- Refinements: 
	- Stochastic hill climbing: generate succesor states at random, and pick the best (using eval fn) - good to avoid local-optimum
	- Random Restart: pick a random starting point and apply hill climbing - then do again a few times and pick the best of each time. 
	- Simualted Annealing: use a decreasing "tempature" to control random selection of candidate succesor (even if it is worse than the prev state). as time goes we make inferior moves less often 
		```while True
			temp = annealing_schedule(temp)
			if temp == 0; return 
			candidate = random_succesor(current)
			diff = evaluation(candidate) - evaluation(current)
			if diff > 0; current = candidate
			else if rand() < e^(diff/temp); current = candidate```

	- Beam Search: search k states at once: 
		- psuedocode: 
			- pick k starting states
			- generate and evaluate each states succesor 
			- pick best 2k states
			- check if one is at the goal 
			- loop if not

- Evolutionary Computing (genetic algorthims)
	- uses biological evolution as model for state search 
	- use survivial of the fittest to get the best solution from a population of solutions
	- encode each candidate solution as a "gene" ie in geneitc code (genetic encoding)
	- psuedocode: 
		- create popualtion of solutions
		- evaluate fitness for each solution in pop
		- pick random set of population called parents
		- "breed" parents to generate childen (ie new solutions) 
			- use mutation and reomcbination (crossover) at random to create new soltuions from old ones 

	- genetic encoding - gene -> linear oderding of symbols in state space - some sort of array that captures the key properties of a solution - influences succesor states - no succesor functions in genetic alg
	- phenotype refers to the solution itself (decoded from "gene")
	- crossover and mutation must match encoding (ie guessian nose for real numbers)
	- can encode mutation rate or crossover rate in the gene itself 
	- fitness score should be higher if better else use 1/1+f 
	- Picking parents: 
		- Roullete wheel: pick parents randomly but higher fitness gives better odds to be selected
		- Tournament: pick n individuals at random and pick the one with the highest fitness. 
- Diff types of Evolutionary computing refer to diff types of encodings: 
	- genetic alg: linear encoding (array)
	- genetic programing: tree (lisp, symbolic)
	- evolution programming: finitie state machine `

